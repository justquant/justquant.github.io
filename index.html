<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Justquant</title>
  <meta name="author" content="Justquant">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="Justquant"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Justquant" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]--><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">Justquant</a></h1>
  <h2><a href="/">尽宽</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper">
  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-11-29T08:17:22.000Z"><a href="/2016/11/29/python-resources/">2016-11-29</a></time>
      
      
  
    <h1 class="title"><a href="/2016/11/29/python-resources/">python-resources</a></h1>
  

    </header>
    <div class="entry">
      
        <h2 id="pandas学习资源"><a href="#pandas学习资源" class="headerlink" title="pandas学习资源"></a>pandas学习资源</h2><ul>
<li>一篇很好的透视表的例子(pivot table)<br><a href="http://python.jobbole.com/81212/" target="_blank" rel="external">http://python.jobbole.com/81212/</a></li>
</ul>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-11-22T06:25:01.000Z"><a href="/2016/11/22/scikitleanr-preparing-data/">2016-11-22</a></time>
      
      
  
    <h1 class="title"><a href="/2016/11/22/scikitleanr-preparing-data/">scikit-learn笔记——准备数据</a></h1>
  

    </header>
    <div class="entry">
      
        <blockquote>
<p>种一棵树最好的时间，是十年前和现在</p>
</blockquote>
<p>终于决定踏入机器学习的世界。只为了那个梦想：养一个机器人在金融市场挖矿，然后去环游世界。</p>
<p>选择scikit-learn，是因为python已经成为事实上的数据分析和挖掘的语言，是很多数据科学家的首选。而且scikit-learn还可以与Spark集成，感谢Databricks的工程师。</p>
<p>参考：<a href="https://databricks.com/blog/2016/02/08/auto-scaling-scikit-learn-with-apache-spark.html" target="_blank" rel="external">Auto-scaling scikit-learn with Apache Spark</a></p>
<p>俗话说：兵马未动粮草先行。要学习scikit-learn，首先需要有数据。作为学习，Scikit-learn提供了三种方式获取数据。</p>
<h1 id="内部自带的数据"><a href="#内部自带的数据" class="headerlink" title="内部自带的数据"></a>内部自带的数据</h1><p>scikit-learn包自带的数据在datasets模块当中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div></pre></td></tr></table></figure>
<p>在IPython中，通过输入datasets.*?会列出datasets包含的所有API。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">boston = d.load_boston()</div><div class="line">print(boston.DESCR)</div></pre></td></tr></table></figure>
<p>内部数据的接口用：datasets.load_*?()</p>
<h1 id="外部数据"><a href="#外部数据" class="headerlink" title="外部数据"></a>外部数据</h1><p>datasets模块也包含了API获取外部的数据。这些API以 fetch_*? 开头。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">housing = datasets.fetch_california_housing()</div></pre></td></tr></table></figure>
<h1 id="造数据"><a href="#造数据" class="headerlink" title="造数据"></a>造数据</h1><p>除了已有的数据，scikit-learn还提供了丰富的API来生成少量的数据。</p>
<p>这些接口都以datasets.make_*?开头。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">datasets.make_regression(<span class="number">1000</span>,<span class="number">10</span>,<span class="number">5</span>,<span class="number">2</span>,<span class="number">1.0</span>)</div></pre></td></tr></table></figure>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-11-21T03:00:20.000Z"><a href="/2016/11/21/cli-construction-tools/">2016-11-21</a></time>
      
      
  
    <h1 class="title"><a href="/2016/11/21/cli-construction-tools/">使用scopt解析Spark参数</a></h1>
  

    </header>
    <div class="entry">
      
        <p>最近在开发一个Spark程序，需要解析自定义的参数。经过一番搜索，选定了两个工具：</p>
<ul>
<li><a href="https://commons.apache.org/proper/commons-cli/" target="_blank" rel="external">Apache Commons CLI</a></li>
<li><a href="https://github.com/scopt/scopt" target="_blank" rel="external">Scopt, Simple scala command line options parsing</a></li>
</ul>
<p>从官方介绍来看，Apache Commons CLI功能比较强大，可以支持丰富的语法。但最终我们还是选定了scopt，主要是因为它是scala写的，而且我们只需要一个简单的解析器。</p>
<h2 id="第一步：将scopt添加到sbt工程的依赖"><a href="#第一步：将scopt添加到sbt工程的依赖" class="headerlink" title="第一步：将scopt添加到sbt工程的依赖"></a>第一步：将scopt添加到sbt工程的依赖</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">libraryDependencies += &quot;com.github.scopt&quot; %% &quot;scopt&quot; % &quot;3.5.0&quot;</div></pre></td></tr></table></figure>
<h2 id="第二步：定义一个-case-class"><a href="#第二步：定义一个-case-class" class="headerlink" title="第二步：定义一个 case class"></a>第二步：定义一个 case class</h2><p>一般来说，要给每一个参数赋予一个默认值。因为scopt在parse参数的时候，是从一个空的对象开始的。是使用Immutable的方式，每parse一个参数，都是在原来的case class的对象上复制一个新的对象，并赋予该参数对应的值。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">case class Configuration(input:String = &quot;&quot;, output:String = &quot;&quot;, number:Int = 0, other:String = &quot;&quot;)</div></pre></td></tr></table></figure>
<h2 id="第三步：定义parser，这里一般使用scopt-OptionParser。这样可以指定一个参数是必须的还是可选的。"><a href="#第三步：定义parser，这里一般使用scopt-OptionParser。这样可以指定一个参数是必须的还是可选的。" class="headerlink" title="第三步：定义parser，这里一般使用scopt.OptionParser。这样可以指定一个参数是必须的还是可选的。"></a>第三步：定义parser，这里一般使用scopt.OptionParser。这样可以指定一个参数是必须的还是可选的。</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">object ScoptDemo &#123;</div><div class="line"></div><div class="line">  def main(args: Array[String]): Unit = &#123;</div><div class="line">    val parser = new scopt.OptionParser[Configuration](&quot;scopt demo&quot;) &#123;</div><div class="line">      head(&quot;scopt&quot;,&quot;3.x&quot;)</div><div class="line"></div><div class="line">      opt[String](&apos;i&apos;,&quot;input&quot;).required().action((x,config) =&gt; config.copy(input = x)).text(&quot;input directory&quot;)</div><div class="line"></div><div class="line">      opt[String](&apos;o&apos;, &quot;output&quot;).required().action((x,config) =&gt; config.copy(output = x)).text(&quot;output directory&quot;)</div><div class="line"></div><div class="line">      opt[String](&apos;n&apos;, &quot;num&quot;).optional().action((x,config) =&gt; config.copy(number = x.toInt)).text(&quot;number of inputs&quot;)</div><div class="line"></div><div class="line">      opt[String](&apos;t&apos;,&quot;other&quot;).optional().action((x,config) =&gt; config.copy(other = x)).text(&quot;others&quot;)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    parser.parse(args, Configuration()) match &#123;</div><div class="line">      case Some(configuration) =&gt; println(configuration)</div><div class="line">      case None =&gt; println(&quot;cannot parse args&quot;)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>以下是直接执行该程序（不传入任何参数）的错误提示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">cannot parse args</div><div class="line">Error: Missing option --input</div><div class="line">Error: Missing option --output</div><div class="line">scopt 3.x</div><div class="line">Usage: scopt demo [options]</div><div class="line"></div><div class="line">  -i, --input &lt;value&gt;   input directory</div><div class="line">  -o, --output &lt;value&gt;  output directory</div><div class="line">  -n, --num &lt;value&gt;     number of inputs</div><div class="line">  -t, --other &lt;value&gt;   others</div></pre></td></tr></table></figure></p>
<p>而执行传入参数列表：-i input1 -o output2 -n 10 -t “no others”，得到的输出为：</p>
<p>Configuration(input1,output2,10,no others)</p>
<p>可见scopt还是非常友好的。scopt还支持其它的功能，例如：</p>
<ul>
<li>对输入的参数进行校验</li>
<li>支持command。command还可以自带自己的参数</li>
</ul>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-11-21T02:44:15.000Z"><a href="/2016/11/21/spark-resources/">2016-11-21</a></time>
      
      
  
    <h1 class="title"><a href="/2016/11/21/spark-resources/">Spark学习资源</a></h1>
  

    </header>
    <div class="entry">
      
        <p><strong>持续更新中…</strong></p>
<h1 id="Gitbook"><a href="#Gitbook" class="headerlink" title="Gitbook"></a>Gitbook</h1><p><a href="https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/index.html" target="_blank" rel="external">Databricks Knowledge Base</a><br><a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/" target="_blank" rel="external">Mastering Apache Spark 2.0</a></p>
<h1 id="网站"><a href="#网站" class="headerlink" title="网站"></a>网站</h1><p><a href="http://spark.apache.org/" target="_blank" rel="external">Spark官网</a></p>
<h1 id="视频"><a href="#视频" class="headerlink" title="视频"></a>视频</h1><h1 id="书籍"><a href="#书籍" class="headerlink" title="书籍"></a>书籍</h1>
      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-11-20T02:44:15.000Z"><a href="/2016/11/20/spark-performance-tuning/">2016-11-20</a></time>
      
      
  
    <h1 class="title"><a href="/2016/11/20/spark-performance-tuning/">Spark性能调优</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Spark是一个优秀的弹性分布式计算系统，但是它不是一个“神奇”的分布式系统，你需要正确的使用它提供的API并且合理的分配资源。<br>本文介绍的技巧主要是针对原生的RDD API。Spark SQL和Dataset在引擎层面提供了不同程度的优化，以后的文章再专门介绍。</p>
<h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><p>在Spark体系里，有两个层面的概念需要掌握。</p>
<ul>
<li>开发层面：RDD, Transformation, Action等</li>
<li>执行层面：Job、Stage、Task、Executor、Shuffle、Partition</li>
</ul>
<p>那么在进行程序优化的时候，也要针对这两个层面进行优化。不仅要正确的编写Spark的程序，还要在执行程序的时候，正确的分配资源。</p>
<h1 id="Spark如何运行你的程序"><a href="#Spark如何运行你的程序" class="headerlink" title="Spark如何运行你的程序"></a>Spark如何运行你的程序</h1><p>一个典型的Spark应用，都有一个Driver和多个Executor。Driver是一个中心调度器，负责把Job分配给各个executor。executor分布<br>在集群的各个机器上，负责执行任务。</p>
<p><img src="/images/spark-tuning-f1.png" alt="Spark执行图"></p>
<p>RDD的tranformation不会触发程序的执行，只有Action才会。一个Action API对应一个Job的执行。</p>
<p>一个Job分成很多个Stage，每个Stage被划分成很多个Task，每个Task运行的代码是一样的，只是他们操作在不同分区（partition）的数据上。这些Task会被分派<br>到各个Executor上去执行，一个Executor可以同时执行多个Task，取决于Executor能分配的资源(CPU,内存).</p>
<p>在一个Stage内部的所有Transformation操作都不会触发Shuffle。Stage之间会发生shuffle。（反过来想，如果两个stage之间没有shuffle，那么可以合成一个Stage ^-^)</p>
<p>在了解了Spark的运行机制后，那么就可以针对其中的每个环节进行优化。</p>
<h1 id="优化1-减少不必要的Shuffle"><a href="#优化1-减少不必要的Shuffle" class="headerlink" title="[优化1] 减少不必要的Shuffle"></a>[优化1] 减少不必要的Shuffle</h1><p>Shuffle的意思是洗牌。就像洗扑克牌一样，把数据重新排列分部。注意Shuffle很多时候是必可避免的，甚至有时候为了提高性能，需要先进行shuffle把数据重新分配到更多的分区以利用集群的计算资源。<br>但是我们一定要努力减少<strong>不必要</strong>的Shuffle操作，因为Shuffle会把数据写到硬盘，然后供下一个stage读取。这会大大增加网络和IO的消耗。记住，在大数据的世界里，“计算”是比“数据”更便宜的资源。<br>要尽量移动计算，而不是数据。</p>
<p>而不必要的Shuffle的产生，往往和错误的使用API有关。</p>
<h2 id="Shuffle是如何产生的"><a href="#Shuffle是如何产生的" class="headerlink" title="Shuffle是如何产生的"></a>Shuffle是如何产生的</h2><p>RDD是Spark的核心。RDD是一种分布式的数据结构，但RDD提供很多API来操作数据。</p>
<p>有一些API，例如filter(), map()， 对于一个分区的数据，可以直接进行操作。这类操作被称为Narrow Transformation (narrow我的理解就是没有夸partition之间的依赖)。<br>而又另外的一些API，例如groupByKey(), reduceByKey()，这些操作需要把相同key的数据都先放到一个partition，以便于能被同一个task执行。这类操作被称为Wide Transformation。<br>这时候就需要进行shuffle。</p>
<p>Here are all RDD Transformation (v2.0)</p>
<p><strong>Narrow Transformation:</strong></p>
<ul>
<li>map</li>
<li>filter</li>
<li>flatMap</li>
<li>mapPartitions</li>
<li>mapPartitionsWithIndex</li>
<li>sample</li>
<li>union</li>
<li>intersection</li>
<li>distinct ??</li>
</ul>
<p><strong> Wide Transformation:</strong></p>
<ul>
<li>groupByKey</li>
<li>reduceByKey</li>
<li>aggregateByKey</li>
<li>sortByKey</li>
<li>join (Not always)</li>
<li>cogroup</li>
<li>cartesian</li>
<li>repartition</li>
<li>repartitionAndSortWithPartitions</li>
<li>coalesce</li>
</ul>
<p><em>例子</em></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">sc.textFile(<span class="string">"someFile.txt"</span>).</div><div class="line">  map(mapFunc).</div><div class="line">  flatMap(flatMapFunc).</div><div class="line">  filter(filterFunc).</div><div class="line">  count()</div></pre></td></tr></table></figure>
<p>这段代码里，有一个action：count()，因此会产生一个job。这个job只包含一个stage，所有的tranformation都没有操作涉及到shuffle。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> tokenized = sc.textFile(args(<span class="number">0</span>)).flatMap(_.split(' '))</div><div class="line"><span class="keyword">val</span> wordCounts = tokenized.map((_, <span class="number">1</span>)).reduceByKey(_ + _)</div><div class="line"><span class="keyword">val</span> filtered = wordCounts.filter(_._2 &gt;= <span class="number">1000</span>)</div><div class="line"><span class="keyword">val</span> charCounts = filtered.flatMap(_._1.toCharArray).map((_, <span class="number">1</span>)).</div><div class="line">  reduceByKey(_ + _)</div><div class="line">charCounts.collect()</div></pre></td></tr></table></figure>
<p>这段代码有一个action：collect()。它有两个wide transformation: reduceByKey()， 这两个reduceByKey()操作把job划分为3个stage，第一个和第二个stage要做shuffle的操作。</p>
<ol>
<li>第一个stage：textFile –&gt; flatMap –&gt; map –&gt; reduceByKey</li>
<li>第二个stage: filter –&gt; flatMap –&gt; map –&gt; reduceByKey</li>
<li>第三个stage: count</li>
</ol>
<h2 id="正确的使用API"><a href="#正确的使用API" class="headerlink" title="正确的使用API"></a>正确的使用API</h2><p>条条大路通罗马，但不是每条道路都省时省心。在使用Spark的API进行数据运算时，往往有很多种做法，但不是每种做法都是高效的。</p>
<p>总的原则：避免shuffle或者减少shuffle的数据量。</p>
<ol>
<li>在使用reduce能使数据减少的情况下，使用reduceBykey()而不是groupByKey()</li>
</ol>
<p>rdd.groupByKey().mapValues(<em>.sum) 会将整个RDD先shuffle然后再对每个partition的数据进行相加<br>rdd.reduceByKey(</em> + _) 会将每个partition的数据先加在一起，然后再shuffle</p>
<p>这样能显著减少shuffle的数据量。但是如果reduce的算子不是 <em> + </em>， 而是某些让数据数量不变甚至增长的情况，则不适用。</p>
<ol>
<li>如果目标RDD的类型是一个集合类型，尽量使用mutable的集合类型</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">rdd.map(kv =&gt; (kv._1, new Set[String]() + kv._2))</div><div class="line">    .reduceByKey(_ ++ _)</div></pre></td></tr></table></figure>
<p>这种写法会为每一个map操作新建一个Set，非常损耗性能。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">val zero = new collection.mutable.Set[String]()</div><div class="line">rdd.aggregateByKey(zero)(</div><div class="line">    (set, v) =&gt; set += v,</div><div class="line">    (set1, set2) =&gt; set1 ++= set2)</div></pre></td></tr></table></figure>
<p>这种写法没有每次new一个Set的开销，更加高效。</p>
<ol>
<li>当数据量很少时，使用Broadcast而不是建成一个RDD。</li>
</ol>
<p>通常做法是把一个集合变成一个哈希表，然后广播到各个executor，然后在做Map操作时，直接通过Key来引用对应的值。</p>
<h2 id="Partition对shuffle的影响"><a href="#Partition对shuffle的影响" class="headerlink" title="Partition对shuffle的影响"></a>Partition对shuffle的影响</h2><p>Spark总是竭尽所能减少shuffle的发生。例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">rdd1 = someRdd.reduceByKey(...)</div><div class="line">rdd2 = someOtherRdd.reduceByKey(...)</div><div class="line">rdd3 = rdd1.join(rdd2)</div></pre></td></tr></table></figure>
<p>如果没有优化，那么产生rdd1需要一次shuffle，产生rdd2需要一次shuffle，产生rdd3需要两次shuffle （rdd1和rdd2各自shuffle一次）</p>
<p>rdd1和rdd2都使用reduceByKey()并且没有指定partitioner，那么他们使用的是默认的partitioner, 如果他们的partition数据也是一样的，<br>同样的key在两个RDD中都只能位于各自的某个partition中，rdd3的每个partition的数据都来源于rdd1和rdd2的一个partition。因此总共shuffle了两次。</p>
<p>但是如果rdd1和rdd2两个的partition数目不一样，或者数据一样，但是他们的partitioner不一样，那么就总共需要三次次shuffle。<br>第一次: someRdd进行shuffle然后得到rdd1<br>第二次: someOtherRDD进行shuffle然后得到rdd2<br>第三次：将partition数目较少的RDD进行shuffle，使得和partition数据较多的RDD partition数目一样，然后再进行join。</p>
<h1 id="优化2：合理分配计算资源"><a href="#优化2：合理分配计算资源" class="headerlink" title="优化2：合理分配计算资源"></a>优化2：合理分配计算资源</h1><p>除了尽量避免shuffle的产生或者减少shuffle的数据量，还需要为每个executor分配正确的资源。Spark管理的资源主要有CPU和内存。<br>对于网络和IO等资源，Spark并没有提供主动的管理。</p>
<p>这里主要讲基于Yarn的资源分配。</p>
<h2 id="CPU内核分配"><a href="#CPU内核分配" class="headerlink" title="CPU内核分配"></a>CPU内核分配</h2><p>Spark可以给每个executor指定其最大可以使用的CPU核数。</p>
<p>有三个地方可以指定：</p>
<ul>
<li>spark-submit,spark-shell: –executor-cores</li>
<li>spark-defaults.conf: spark.executor.cores</li>
<li>SparkConf</li>
</ul>
<p>当Spark和Yarn结合时，Spark能申请的CPU资源还要受限于Yarn。</p>
<p>yarn.nodemanager.resource.cpu-vcores 参数控制每个Yarn容器能使用的CPU数目。</p>
<p>当Spark申请3个核时，实际上Spark是向Yarn申请了3个vcores.</p>
<h2 id="内存分配"><a href="#内存分配" class="headerlink" title="内存分配"></a>内存分配</h2><p>有三个地方可以指定：</p>
<ul>
<li>spark-submit,spark-shell: –executor-memory</li>
<li>spark-defaults.conf: spark.executor.memory</li>
<li>SparkConf</li>
</ul>
<p>Spark向Yarn申请内存时比申请CPU要复杂。</p>
<ul>
<li>–executor-memory 控制的是executor的堆大小，但是每个executor还需要使用额外的内存空间来做缓存。</li>
<li>spark.yarn.executor.memoryOverhead 用于控制向Yarn申请的额外的内存。它的默认值等于：max(384, 0.07 * spark.executor.memory)</li>
<li>yarn.scheduler.minimum-allocation-mb 控制一个yarn容器最小分配的内存</li>
</ul>
<p><img src="/images/spark-tuning2-f1.png" alt="Spark内存申请"></p>
<h2 id="分配原则"><a href="#分配原则" class="headerlink" title="分配原则"></a>分配原则</h2><ol>
<li>Spark的application master也需要占用资源。在yarn-client模式下，默认占用1个CPU核和1G内存。在yarn-cluster模式下，application<br>master就是driver，由于application master同时也可能跑executor，因此要通过 –driver-memory 和 –driver-cores来为driver预留<br>足够的程序。</li>
<li>不应该为一个executor分配太多的内存，这样反而会引起垃圾回收的延时。一般一个executor分配的内存最大不超过64G</li>
<li>一个executor一般分配不超过5个CPU核心，太多的话可能会使得hadoop写入文件阻塞（希望后来没有这个问题 !)</li>
<li>尽量不要分配一个executor只有一个CPU内核，然后在一台机器上创建很多个executor。主要有两个坏处：<ul>
<li>broadcast是建立在executor上的，太多executor导致太多的广播变量</li>
<li>每个executor都会占用一个额外的内存开销</li>
</ul>
</li>
<li>永远要预留一个CPU内核和一定的内存供操作系统使用</li>
</ol>
<h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><p>假设一个集群有6台机器，每台机器有16核，64G内存，那么该如何分配资源呢？</p>
<p>首先给yarn分配资源：</p>
<p>yarn.nodemanager.resource.cpu-vcores 15<br>yarn.nodemanager.resource.memory-mb  63G</p>
<p>要为系统进程预留1个核和1G的内存。</p>
<p>然后给spark的executor分配CPU和内存。一种最直接的分配方案：</p>
<p>–num-executors=6  –executor-cores=15 –executor-memory=63G</p>
<p>也就是每天机器创建一个executor，每个executor占用15个核心，63G内存。但这是一个不可行的方案。首先每个executor分配15个核心，会导致HDFS<br>被阻塞，而且一个executor占用63G内存，加上额外的开销就超过63G了。</p>
<p>一种优化的方案为：</p>
<p>–num-executors=17  –executor-cores=5 –executor-memory=19</p>
<ul>
<li>每个executor占用5个核心。–executor-cores=5</li>
<li>每个机器可以有 15 / 5 = 3 个executor, 6台机器一共可以创建18个executor，但是我们要除去application master， 因此共有18-1=17个executor</li>
<li>每天机器上的3个executor，每个executor可以分配到 63 / 3 = 21 G内存，但是 21G应该是包含了额外的开销的，假设额外开销为 0.07 <em> X<br>0.07 </em> X + X = 21, X = 19.6, 向下舍去，为19G</li>
</ul>
<p>这样的分配不仅可以充分利用资源，而且一般不会出现内存溢出的情况。</p>
<h1 id="优化三：使用更高效的数据存储"><a href="#优化三：使用更高效的数据存储" class="headerlink" title="优化三：使用更高效的数据存储"></a>优化三：使用更高效的数据存储</h1><p>例如使用parquet替代CSV，JSON，使用KryoSerializer替代默认的Java Serializer。这里不做重点介绍。</p>
<h1 id="优化四：增加并行度"><a href="#优化四：增加并行度" class="headerlink" title="优化四：增加并行度"></a>优化四：增加并行度</h1><p>一般来说，在一个Stage里，task的数目和父亲RDD的partition数据是一样的 ，产生的子RDD的partition数目也是一样的。</p>
<p>但是有些操作，可以改变子RDD的partition数目：</p>
<ul>
<li>coalesce可以将父亲RDD的分区数目压缩</li>
<li>union操作产生的RDD分区数目是两个父亲RDD分区的和。</li>
<li>catesian产生的RDD分区数据是两个父亲RDD分区的乘积。</li>
</ul>
<p>分区的数据，决定了Job并行执行的程度。如果有100机器，但是数据只有2个分区，那么一次就只有2个task在执行，其它机器都在空转。</p>
<p>分区太少，会使得单个task要执行的数据过多，占用的时间和空间也较大。那么到底分多少个分区合适呢？</p>
<p>第一种办法就是不断的尝试逼近：找到父亲RDD的分区数目，然后不断乘以1.5，知道发现性能无法获得提升为止。但是这种办法在显示中不太可行，<br>因为你不太可能一次次的去跑同样的job。</p>
<p>另外一种尝试就是，根据系统的CPU，内存结合程序的特点来大概计算，但是很难量化。</p>
<p>总的原则就是：多些分区要比少分区要好，因为在spark里创建task是很便宜的。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><a href="http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/</a></li>
<li><a href="http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/" target="_blank" rel="external">http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/</a></li>
</ul>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-11-18T02:44:15.000Z"><a href="/2016/11/18/hello-world/">2016-11-18</a></time>
      
      
  
    <h1 class="title"><a href="/2016/11/18/hello-world/">你好，世界</a></h1>
  

    </header>
    <div class="entry">
      
        <p>在我学习每一门编程语言，第一个程序都是Hello World! 当我决定重新开始写博客，我想第一遍文章也是 <strong>你好，世界</strong>。</p>
<p>为什么想写博客？想安静。想回忆，想分享。</p>
<p>想静下来。常常有一种感觉，就是明天昨天发生的事情，却只有一个模糊的影响，很多细节都记不清楚。生活的脚步匆匆，灵魂一直在奔忙。</p>
<p>我想，我需要慢下来。而写作，在构思的过程中，能让我的思绪静下来，让我收获宁静。</p>
<p>想回忆，就是想着，有一天自己回过头去看看自己走过的路，有一个地方，帮我存着。</p>
<p>想分享。自己在IT领域工作已经有10个年头了，虽然不是什么大牛，却多多少少一些经验可以去分享。只要能惠及一个人，也就知足了。</p>
<p>找了很多博客例如CSDN等，最后还是自己搭建。不求很多人知道，我只想有那么一个角落，让我自己静静的与自己对话。</p>
<p>就这样，我相遇了Hexo。感谢开源世界哪些默默奉献的人，你们的分享这个世界更美好。</p>
<p>最后为什么把博客取名为Justquant, 因为我有一个梦想，或许是一个白日梦。就是能成为一名宽客，养一个机器人，让它为我在金融市场挖矿，而我，就去环游世界。</p>
<p><strong>感谢<a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! 感谢<a href="https://github.com/" target="_blank" rel="external">Github</a></strong></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>





<nav id="pagination">
  
  
  <div class="clearfix"></div>
</nav></div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="搜索">
    <input type="hidden" name="q" value="site:justquant.github.io">
  </form>
</div>

  

  
<div class="widget tag">
  <h3 class="title">标签</h3>
  <ul class="entry">
  
    <li><a href="/tags/MACD/">MACD</a><small>0</small></li>
  
    <li><a href="/tags/Resource/">Resource</a><small>1</small></li>
  
    <li><a href="/tags/Spark/">Spark</a><small>2</small></li>
  
    <li><a href="/tags/Tool/">Tool</a><small>0</small></li>
  
    <li><a href="/tags/macd/">macd</a><small>0</small></li>
  
    <li><a href="/tags/python/">python</a><small>1</small></li>
  
    <li><a href="/tags/scala/">scala</a><small>1</small></li>
  
    <li><a href="/tags/scikit-learn/">scikit-learn</a><small>1</small></li>
  
    <li><a href="/tags/机器学习/">机器学习</a><small>1</small></li>
  
    <li><a href="/tags/统计学/">统计学</a><small>0</small></li>
  
    <li><a href="/tags/股票/">股票</a><small>0</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2017 Justquant
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

</body>
</html>
